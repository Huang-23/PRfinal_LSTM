{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huang-23/PRfinal_LSTM/blob/main/nlp_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJbNTvV4S9du",
        "outputId": "2a7f407a-87f6-4990-ce08-0fd79a48ed3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIfDznNENMST"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import jieba.posseg as pseg\n",
        "import os\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn0I-tEbATrj"
      },
      "source": [
        "# 讀檔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcy0AZf7NMSY"
      },
      "source": [
        "TRAIN_CSV_PATH = '/content/drive/MyDrive/nlp_final/train.csv'\n",
        "TEST_CSV_PATH = '/content/drive/MyDrive/nlp_final/test.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "BYQ2t_E4NMSY",
        "outputId": "15ca0947-c531-4ed5-80f4-3b0ad78f1de5"
      },
      "source": [
        "train = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\n",
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tid1</th>\n",
              "      <th>tid2</th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>title1_en</th>\n",
              "      <th>title2_en</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
              "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
              "      <td>There are two new old-age insurance benefits f...</td>\n",
              "      <td>Police disprove \"bird's nest congress each per...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
              "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
              "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
              "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
              "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    tid1  tid2  ...                                          title2_en      label\n",
              "id              ...                                                              \n",
              "0      0     1  ...  Police disprove \"bird's nest congress each per...  unrelated\n",
              "3      2     3  ...  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated\n",
              "1      2     4  ...  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "dw-StXfNNMSZ",
        "outputId": "68f10c86-8812-495d-a135-902985e7ee79"
      },
      "source": [
        "cols = ['title1_zh', \n",
        "        'title2_zh', \n",
        "        'label']\n",
        "train = train.loc[:, cols]\n",
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
              "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            title1_zh                  title2_zh      label\n",
              "id                                                                         \n",
              "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
              "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
              "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_svaaMFNMSZ"
      },
      "source": [
        "# 文本分詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7a1M-yYNMSa"
      },
      "source": [
        "def jieba_tokenizer(text):\n",
        "    words = pseg.cut(text)\n",
        "    return ' '.join([\n",
        "        word for word, flag in words if flag != 'x'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoSeebF1NMSb",
        "outputId": "b8dab77e-42b2-44f9-d8f0-97496f99399d"
      },
      "source": [
        "train.isna().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title1_zh    False\n",
              "title2_zh     True\n",
              "label        False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJV6iwY9NMSb",
        "scrolled": true,
        "outputId": "91e0bb62-443e-4d89-d6d3-c45811826808"
      },
      "source": [
        "train.title2_zh.fillna('UNKNOWN', inplace=True)\n",
        "train.isna().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title1_zh    False\n",
              "title2_zh    False\n",
              "label        False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkVr_fkFNMSc",
        "outputId": "5adf439b-8129-4992-dd9b-bbd1f1d36fe5"
      },
      "source": [
        "def process(data):\n",
        "    res = data.apply(jieba_tokenizer)\n",
        "    return res\n",
        "\n",
        "def check_merge_idx(data, res):\n",
        "    assert((data.index == res.index).all(), 'Something error when merge data')\n",
        "\n",
        "def parallelize(data, func):\n",
        "    from multiprocessing import cpu_count, Pool\n",
        "    cores = partitions = cpu_count()\n",
        "    data_split = np.array_split(data, partitions)\n",
        "    pool = Pool(cores)\n",
        "    res = pd.concat(pool.map(func, data_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    check_merge_idx(data, res)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-9-2085024a8821>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert((data.index == res.index).all(), 'Something error when merge data')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx91rrs1NMSc",
        "outputId": "b0211404-0520-49f0-c691-4ba8a6407c87"
      },
      "source": [
        "np.all(train.index == train.title1_zh.index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn3gER8oNMSc",
        "scrolled": true,
        "outputId": "c804c38f-20bc-4f1f-eabf-61da8d32f817"
      },
      "source": [
        "print(\"start to training\")\n",
        "train['title1_tokenized'] = parallelize(train.loc[:, 'title1_zh'], process)\n",
        "train['title2_tokenized'] = parallelize(train.loc[:, 'title2_zh'], process)\n",
        "#train.to_csv('/content/drive/MyDrive/nlp_final/tokenized_train.csv',index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start to training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.707 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Loading model cost 1.777 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 1.261 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Loading model cost 1.252 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "aGVwZsRJNMSd",
        "scrolled": true,
        "outputId": "c15f14ae-d14a-46d4-ee6e-db3a964a24ea"
      },
      "source": [
        "train.loc[:, [\"title1_zh\", \"title1_tokenized\"]].head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title1_tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
              "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
              "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\"吃榴莲的禁忌,吃错会致命!</td>\n",
              "      <td>吃 榴莲 的 禁忌 吃 错会 致命</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"旅行青蛙？居然是一款\"\"生育意愿测试器”！大家还是玩\"\"珠宝V课\"\"吧\"</td>\n",
              "      <td>旅行 青蛙 居然 是 一款 生育 意愿 测试 器 大家 还是 玩 珠宝 V 课 吧</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
              "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                title1_zh                                 title1_tokenized\n",
              "id                                                                                        \n",
              "0           2017养老保险又新增两项，农村老人人人可申领，你领到了吗         2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
              "3       \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "1       \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "2       \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "9                    \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油                       用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
              "4       \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "6                          \"吃榴莲的禁忌,吃错会致命!                                吃 榴莲 的 禁忌 吃 错会 致命\n",
              "5       \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "7   \"旅行青蛙？居然是一款\"\"生育意愿测试器”！大家还是玩\"\"珠宝V课\"\"吧\"        旅行 青蛙 居然 是 一款 生育 意愿 测试 器 大家 还是 玩 珠宝 V 课 吧\n",
              "8                    \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油                       用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "aHgUsBhMNMSd",
        "outputId": "dc9dad6f-c892-469a-a068-e223dcf30dca"
      },
      "source": [
        "train.loc[:, [\"title2_zh\", \"title2_tokenized\"]].head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>title2_tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
              "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
              "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
              "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
              "      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
              "      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
              "      <td>深圳 GDP 首 超 香港 统计局 辟谣 未 超 但 差距 再度 缩小</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>榴莲不能和什么一起吃 与咖啡同吃诱发心脏病\"\"</td>\n",
              "      <td>榴莲 不能 和 什么 一起 吃 与 咖啡 同 吃 诱发 心脏病</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>深圳GDP首超香港？辟谣：未超但差距再度缩小</td>\n",
              "      <td>深圳 GDP 首 超 香港 辟谣 未 超 但 差距 再度 缩小</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>咸宁一家店的蛋糕含有“棉花”？崇阳多部门联合辟谣</td>\n",
              "      <td>咸宁 一家 店 的 蛋糕 含有 棉花 崇阳 多 部门 联合 辟谣</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>一颗大蒜就能鉴别地沟油？别闹了！做到下面几点，让您远离地沟油</td>\n",
              "      <td>一颗 大蒜 就 能 鉴别 地沟油 别闹 了 做到 下面 几点 让 您 远离 地沟油</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         title2_zh                           title2_tokenized\n",
              "id                                                                           \n",
              "0         警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京          警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
              "3        深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小         深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
              "1             GDP首超香港？深圳澄清：还差一点点……                   GDP 首 超 香港 深圳 澄清 还 差 一点点\n",
              "2       去年深圳GDP首超香港？深圳统计局辟谣：还差611亿       去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n",
              "9          吃了30年食用油才知道，一片大蒜轻松鉴别地沟油          吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油\n",
              "4        深圳GDP首超香港？统计局辟谣：未超但差距再度缩小        深圳 GDP 首 超 香港 统计局 辟谣 未 超 但 差距 再度 缩小\n",
              "6          榴莲不能和什么一起吃 与咖啡同吃诱发心脏病\"\"            榴莲 不能 和 什么 一起 吃 与 咖啡 同 吃 诱发 心脏病\n",
              "5           深圳GDP首超香港？辟谣：未超但差距再度缩小            深圳 GDP 首 超 香港 辟谣 未 超 但 差距 再度 缩小\n",
              "7         咸宁一家店的蛋糕含有“棉花”？崇阳多部门联合辟谣           咸宁 一家 店 的 蛋糕 含有 棉花 崇阳 多 部门 联合 辟谣\n",
              "8   一颗大蒜就能鉴别地沟油？别闹了！做到下面几点，让您远离地沟油  一颗 大蒜 就 能 鉴别 地沟油 别闹 了 做到 下面 几点 让 您 远离 地沟油"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmt-HyA6NMSe"
      },
      "source": [
        "train.fillna('UNKNOWN', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5QlYtG8Bx95"
      },
      "source": [
        "# 建立字典並將文本轉成數字序列\n",
        "* 將已被斷詞的新聞標題 A 以及新聞標題 B 全部倒在一起\n",
        "* 建立一個空字典\n",
        "* 查看所有新聞標題，裏頭每出現一個字典裡頭沒有的詞彙，就為該詞彙指定一個字典裡頭還沒出現的索引數字，並將該詞彙放入字典\n",
        "* 利用建好的字典，將每個新聞標題裡頭包含的詞彙轉換成數字"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2BF6fp2NMSe"
      },
      "source": [
        "# Keras專門的文字前處理模組\n",
        "import keras\n",
        "MAX_NUM_WORDS = 10000  # 限制字典只能包含10,000個詞彙，以避免字典過於龐大\n",
        "# 將一段文字轉換成一系列的詞彙（Tokens），並為其建立字典\n",
        "tokenizer = keras \\\n",
        "    .preprocessing \\\n",
        "    .text \\\n",
        "    .Tokenizer(num_words=MAX_NUM_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQm3_L7pNMSe",
        "outputId": "f0ed4934-0303-47aa-c5e9-95dbf41bf96a"
      },
      "source": [
        "# 將新聞 A 及新聞 B 的標題全部聚集起來(語料庫)，為它們建立字典\n",
        "corpus_x1 = train.title1_tokenized\n",
        "corpus_x2 = train.title2_tokenized\n",
        "corpus = pd.concat([\n",
        "    corpus_x1, corpus_x2])\n",
        "corpus.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(641104,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "SxYVwuNHNMSf",
        "outputId": "dfcddd91-478e-4a24-f1f3-80e05af1ad95"
      },
      "source": [
        "# 語料庫的一小部分\n",
        "pd.DataFrame(corpus.iloc[:5],\n",
        "             columns=['title'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              title\n",
              "id                                                 \n",
              "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
              "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
              "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A38nF77tNMSf",
        "scrolled": false,
        "outputId": "9b6f4e5b-8766-40b6-c7f2-aec3db38136c"
      },
      "source": [
        "corpus.isna().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cqkpWtsNMSf"
      },
      "source": [
        "# 呼叫tokenizer查看所有文本，並建立一個字典（2&3）\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "# 請tokenizer利用內部生成的字典分別將新聞標題A與B轉換成數字序列\n",
        "x1_train = tokenizer \\\n",
        "    .texts_to_sequences(corpus_x1)\n",
        "x2_train = tokenizer \\\n",
        "    .texts_to_sequences(corpus_x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY1UMqEeNMSg",
        "outputId": "80bd194d-2620-4f78-9834-6628fbd2fdc0"
      },
      "source": [
        "len(x1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320552"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERAk-_OWNMSg",
        "outputId": "58cb6dfb-32bb-482e-beab-d15b7c6a9c75"
      },
      "source": [
        "# x1_train為一個Python list，包含了每一筆假新聞標題A對應的數字序列\n",
        "x1_train[:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5b48CT_NMSh",
        "outputId": "e025e2c9-8d52-420e-e9ba-2dd1261b399b"
      },
      "source": [
        "# 將索引數字對應回本來的詞彙\n",
        "for seq in x1_train[:1]:\n",
        "    print([tokenizer.index_word[idx] for idx in seq])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-9pO0NQNMSh"
      },
      "source": [
        "# 序列的 Zero Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "048mnuZDNMSh"
      },
      "source": [
        "# 長度超過此數字的序列尾巴會被刪掉；而針對原來長度不足的序列則會在詞彙前面補零\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "x1_train = keras \\\n",
        "    .preprocessing \\\n",
        "    .sequence \\\n",
        "    .pad_sequences(x1_train, \n",
        "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "x2_train = keras \\\n",
        "    .preprocessing \\\n",
        "    .sequence \\\n",
        "    .pad_sequences(x2_train, \n",
        "                   maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77j9zmaDNMSi",
        "outputId": "0f0bbd47-f8e2-47b6-c756-0d123fc522e1"
      },
      "source": [
        "x1_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,  217, 1268,   32, 1178, 5967,\n",
              "         25,  489, 2877,  116, 5559,    4, 1850,    2,   13], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV_3UlfoNMSi",
        "outputId": "ef71f622-bbfb-4164-8995-f0db7356cf1a"
      },
      "source": [
        "for seq in x1_train + x2_train:\n",
        "    assert len(seq) == 20\n",
        "    \n",
        "print(\"所有新聞標題的序列長度皆為 20 !\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "所有新聞標題的序列長度皆為 20 !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YjajsLRNMSj"
      },
      "source": [
        "# 將 Label 做 One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq1AN6ULNMSj",
        "outputId": "ebb3dfdf-8b46-4a2a-ce0a-2bc66951ccfa"
      },
      "source": [
        "train.label[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0    unrelated\n",
              "3    unrelated\n",
              "1    unrelated\n",
              "2    unrelated\n",
              "9       agreed\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZaqxWKANMSl",
        "outputId": "5b97b5c3-2f8d-46d8-b5f2-8f1f470a67b3"
      },
      "source": [
        "# 需要一個字典將分類的文字轉換成索引\n",
        "import numpy as np \n",
        "\n",
        "# 定義每一個分類對應到的索引數字\n",
        "label_to_index = {\n",
        "    'unrelated': 0, \n",
        "    'agreed': 1, \n",
        "    'disagreed': 2\n",
        "}\n",
        "\n",
        "# 將分類標籤對應到剛定義的數字\n",
        "y_train = train.label.apply(\n",
        "    lambda x: label_to_index[x])\n",
        "\n",
        "y_train = np.asarray(y_train) \\\n",
        "            .astype('float32')\n",
        "\n",
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5eQGV5BNMSn",
        "outputId": "11c170f0-c43d-4d8c-d69e-db66e21ce336"
      },
      "source": [
        "x1_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,  217, 1268,   32, 1178, 5967,\n",
              "          25,  489, 2877,  116, 5559,    4, 1850,    2,   13],\n",
              "       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n",
              "        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
              "       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n",
              "        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
              "       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n",
              "        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          31,  320, 3372, 3062,    1,   95,   98, 3372, 3062]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYmbl5qsNMSo",
        "outputId": "c2e54768-3811-4153-ccf0-fb954c6befc0"
      },
      "source": [
        "train.label[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0    unrelated\n",
              "3    unrelated\n",
              "1    unrelated\n",
              "2    unrelated\n",
              "9       agreed\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN1rZhFINMSp",
        "outputId": "3c024271-f2bd-4dee-d908-14d3fbe753dd"
      },
      "source": [
        "# 利用 Keras 做 One Hot Encoding\n",
        "# [1, 0, 0] 就代表一組新聞標題 A、B 為 unrelated 的機率等於 100 %\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_train = keras \\\n",
        "    .utils \\\n",
        "    .np_utils \\\n",
        "    .to_categorical(y_train)\n",
        "\n",
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26qBRI0NFzWk"
      },
      "source": [
        "# 切割訓練 & 驗證資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpCG9fGJNMSq"
      },
      "source": [
        "from sklearn.model_selection \\\n",
        "    import train_test_split\n",
        "\n",
        "VALIDATION_RATIO = 0.1  # 決定要將整個訓練資料集的多少比例切出來當作驗證資料集\n",
        "RANDOM_STATE = 9527\n",
        "\n",
        "x1_train, x1_val, \\\n",
        "x2_train, x2_val, \\\n",
        "y_train, y_val = \\\n",
        "    train_test_split(\n",
        "        x1_train, x2_train, y_train, \n",
        "        test_size=VALIDATION_RATIO, \n",
        "        random_state=RANDOM_STATE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERJjmkS3NMSq",
        "outputId": "e5845d91-ee92-4643-cd8c-430f5dac3144"
      },
      "source": [
        "print(\"Training Set\")\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_train: {x1_train.shape}\")\n",
        "print(f\"x2_train: {x2_train.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "\n",
        "print(\"-\" * 10)\n",
        "print(f\"x1_val:   {x1_val.shape}\")\n",
        "print(f\"x2_val:   {x2_val.shape}\")\n",
        "print(f\"y_val :   {y_val.shape}\")\n",
        "print(\"-\" * 10)\n",
        "print(\"Test Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set\n",
            "----------\n",
            "x1_train: (288496, 20)\n",
            "x2_train: (288496, 20)\n",
            "y_train : (288496, 3)\n",
            "----------\n",
            "x1_val:   (32056, 20)\n",
            "x2_val:   (32056, 20)\n",
            "y_val :   (32056, 3)\n",
            "----------\n",
            "Test Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31snDB2INoZ"
      },
      "source": [
        "# 此模型的 Keras 實作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPIc6svNMSm"
      },
      "source": [
        "# 基本參數設置，有幾個分類\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "# 在語料庫裡有多少詞彙\n",
        "MAX_NUM_WORDS = 10000\n",
        "\n",
        "# 一個標題最長有幾個詞彙\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "\n",
        "# 一個詞向量的維度\n",
        "NUM_EMBEDDING_DIM = 256\n",
        "\n",
        "# LSTM 輸出的向量維度\n",
        "NUM_LSTM_UNITS = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P83T2PrfNMSr",
        "outputId": "ac0058ce-10ca-4253-a4f3-d0f4031427c2"
      },
      "source": [
        "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
        "from keras import Input\n",
        "from keras.layers import Embedding,LSTM, concatenate, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# 分別定義 2 個新聞標題 A & B 為模型輸入\n",
        "# 兩個標題都是一個長度為 20 的數字序列\n",
        "top_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "bm_input = Input(\n",
        "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
        "    dtype='int32')\n",
        "\n",
        "# 詞嵌入層（Word Embedding）\n",
        "# 經過詞嵌入層的轉換，兩個新聞標題都變成一個詞向量的序列，而每個詞向量的維度為256\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(\n",
        "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
        "top_embedded = embedding_layer(\n",
        "    top_input)\n",
        "bm_embedded = embedding_layer(\n",
        "    bm_input)\n",
        "\n",
        "# LSTM 層\n",
        "# 兩個新聞標題經過此層後為一個 128 維度向量\n",
        "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
        "top_output = shared_lstm(top_embedded)\n",
        "bm_output = shared_lstm(bm_embedded)\n",
        "\n",
        "# 串接層將兩個新聞標題的結果串接單一向量\n",
        "# 方便跟全連結層相連\n",
        "merged = concatenate(\n",
        "    [top_output, bm_output], \n",
        "    axis=-1)\n",
        "\n",
        "# 全連接層搭配 Softmax Activation\n",
        "# 可以回傳 3 個成對標題\n",
        "# 屬於各類別的可能機率\n",
        "dense =  Dense(\n",
        "    units=NUM_CLASSES, \n",
        "    activation='softmax')\n",
        "predictions = dense(merged)\n",
        "\n",
        "# 模型就是將數字序列的輸入，轉換成3個分類的機率的所有步驟 / 層的總和\n",
        "model = Model(\n",
        "    inputs=[top_input, bm_input], \n",
        "    outputs=predictions)\n",
        "\n",
        "# 看每一層的參數量以及輸出的張量（Tensor）長相\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 256)      2560000     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          197120      embedding[0][0]                  \n",
            "                                                                 embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n",
            "                                                                 lstm[1][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            771         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 2,757,891\n",
            "Trainable params: 2,757,891\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_A2oyqpNMSt",
        "outputId": "b386895e-4055-42ff-a256-d3adf1fde41a"
      },
      "source": [
        "x1_train[:9527].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9527, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkk5ZgkDNMSs"
      },
      "source": [
        "from keras.optimizers import adam_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLG-3mnfNMSs",
        "outputId": "45c53aff-2658-4a23-b5d8-439f6fb001a8"
      },
      "source": [
        "lr = 1e-3\n",
        "opt = adam_v2.Adam(lr=lr, decay=lr/50)\n",
        "# 定義模型的損失函數\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMnRIsh4J23o"
      },
      "source": [
        "# 訓練模型並挑選最好的結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u6k09iCNMSt",
        "outputId": "1adfbee2-e40c-4dc1-9da9-0ea853c94157"
      },
      "source": [
        "# 決定一次要放多少成對標題給模型訓練\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# 決定模型要看整個訓練資料集幾遍\n",
        "NUM_EPOCHS = 500\n",
        "\n",
        "# 實際訓練模型\n",
        "history = model.fit(\n",
        "    # 輸入是兩個長度為 20 的數字序列\n",
        "    x=[x1_train, x2_train], \n",
        "    y=y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    # 每個 epoch 完後計算驗證資料集\n",
        "    # 上的 Loss 以及準確度\n",
        "    validation_data=(\n",
        "        [x1_val, x2_val], \n",
        "        y_val\n",
        "    ),\n",
        "    # 每個 epoch 隨機調整訓練資料集\n",
        "    # 裡頭的數據以讓訓練過程更穩定\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "282/282 [==============================] - 18s 37ms/step - loss: 0.4936 - accuracy: 0.7748 - val_loss: 0.4057 - val_accuracy: 0.8124\n",
            "Epoch 2/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.3682 - accuracy: 0.8335 - val_loss: 0.3825 - val_accuracy: 0.8292\n",
            "Epoch 3/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.3350 - accuracy: 0.8500 - val_loss: 0.3756 - val_accuracy: 0.8340\n",
            "Epoch 4/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.3093 - accuracy: 0.8640 - val_loss: 0.3744 - val_accuracy: 0.8368\n",
            "Epoch 5/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2872 - accuracy: 0.8746 - val_loss: 0.3810 - val_accuracy: 0.8394\n",
            "Epoch 6/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2696 - accuracy: 0.8828 - val_loss: 0.3769 - val_accuracy: 0.8427\n",
            "Epoch 7/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2497 - accuracy: 0.8920 - val_loss: 0.3832 - val_accuracy: 0.8434\n",
            "Epoch 8/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2328 - accuracy: 0.9002 - val_loss: 0.3941 - val_accuracy: 0.8468\n",
            "Epoch 9/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2165 - accuracy: 0.9072 - val_loss: 0.4025 - val_accuracy: 0.8475\n",
            "Epoch 10/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.2015 - accuracy: 0.9150 - val_loss: 0.4180 - val_accuracy: 0.8483\n",
            "Epoch 11/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1870 - accuracy: 0.9214 - val_loss: 0.4321 - val_accuracy: 0.8461\n",
            "Epoch 12/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1749 - accuracy: 0.9264 - val_loss: 0.4530 - val_accuracy: 0.8458\n",
            "Epoch 13/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1626 - accuracy: 0.9323 - val_loss: 0.4702 - val_accuracy: 0.8452\n",
            "Epoch 14/500\n",
            "282/282 [==============================] - 9s 34ms/step - loss: 0.1520 - accuracy: 0.9373 - val_loss: 0.4855 - val_accuracy: 0.8480\n",
            "Epoch 15/500\n",
            "282/282 [==============================] - 9s 34ms/step - loss: 0.1434 - accuracy: 0.9409 - val_loss: 0.5094 - val_accuracy: 0.8500\n",
            "Epoch 16/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1344 - accuracy: 0.9448 - val_loss: 0.5457 - val_accuracy: 0.8493\n",
            "Epoch 17/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1260 - accuracy: 0.9490 - val_loss: 0.5545 - val_accuracy: 0.8480\n",
            "Epoch 18/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1204 - accuracy: 0.9511 - val_loss: 0.5724 - val_accuracy: 0.8485\n",
            "Epoch 19/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1139 - accuracy: 0.9540 - val_loss: 0.5926 - val_accuracy: 0.8510\n",
            "Epoch 20/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1087 - accuracy: 0.9566 - val_loss: 0.6199 - val_accuracy: 0.8492\n",
            "Epoch 21/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.1035 - accuracy: 0.9588 - val_loss: 0.5978 - val_accuracy: 0.8484\n",
            "Epoch 22/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0997 - accuracy: 0.9601 - val_loss: 0.6434 - val_accuracy: 0.8502\n",
            "Epoch 23/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0957 - accuracy: 0.9617 - val_loss: 0.6589 - val_accuracy: 0.8474\n",
            "Epoch 24/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0918 - accuracy: 0.9630 - val_loss: 0.6806 - val_accuracy: 0.8484\n",
            "Epoch 25/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0890 - accuracy: 0.9645 - val_loss: 0.7012 - val_accuracy: 0.8483\n",
            "Epoch 26/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0861 - accuracy: 0.9658 - val_loss: 0.7249 - val_accuracy: 0.8459\n",
            "Epoch 27/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0834 - accuracy: 0.9665 - val_loss: 0.7356 - val_accuracy: 0.8467\n",
            "Epoch 28/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0813 - accuracy: 0.9672 - val_loss: 0.7316 - val_accuracy: 0.8455\n",
            "Epoch 29/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0787 - accuracy: 0.9683 - val_loss: 0.7527 - val_accuracy: 0.8490\n",
            "Epoch 30/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0765 - accuracy: 0.9689 - val_loss: 0.7554 - val_accuracy: 0.8475\n",
            "Epoch 31/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0750 - accuracy: 0.9696 - val_loss: 0.7651 - val_accuracy: 0.8482\n",
            "Epoch 32/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0734 - accuracy: 0.9699 - val_loss: 0.7783 - val_accuracy: 0.8483\n",
            "Epoch 33/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0718 - accuracy: 0.9706 - val_loss: 0.7937 - val_accuracy: 0.8465\n",
            "Epoch 34/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0701 - accuracy: 0.9711 - val_loss: 0.8079 - val_accuracy: 0.8458\n",
            "Epoch 35/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0691 - accuracy: 0.9718 - val_loss: 0.8396 - val_accuracy: 0.8480\n",
            "Epoch 36/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0678 - accuracy: 0.9720 - val_loss: 0.8351 - val_accuracy: 0.8462\n",
            "Epoch 37/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0667 - accuracy: 0.9724 - val_loss: 0.8563 - val_accuracy: 0.8481\n",
            "Epoch 38/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0649 - accuracy: 0.9731 - val_loss: 0.8663 - val_accuracy: 0.8475\n",
            "Epoch 39/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0633 - accuracy: 0.9734 - val_loss: 0.8822 - val_accuracy: 0.8451\n",
            "Epoch 40/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0630 - accuracy: 0.9734 - val_loss: 0.8719 - val_accuracy: 0.8474\n",
            "Epoch 41/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0629 - accuracy: 0.9734 - val_loss: 0.8972 - val_accuracy: 0.8484\n",
            "Epoch 42/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0613 - accuracy: 0.9740 - val_loss: 0.9208 - val_accuracy: 0.8459\n",
            "Epoch 43/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0601 - accuracy: 0.9746 - val_loss: 0.9438 - val_accuracy: 0.8469\n",
            "Epoch 44/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0592 - accuracy: 0.9747 - val_loss: 0.9399 - val_accuracy: 0.8484\n",
            "Epoch 45/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0589 - accuracy: 0.9743 - val_loss: 0.8957 - val_accuracy: 0.8464\n",
            "Epoch 46/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0581 - accuracy: 0.9749 - val_loss: 0.9496 - val_accuracy: 0.8481\n",
            "Epoch 47/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0591 - accuracy: 0.9747 - val_loss: 0.9424 - val_accuracy: 0.8481\n",
            "Epoch 48/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0581 - accuracy: 0.9748 - val_loss: 0.9511 - val_accuracy: 0.8474\n",
            "Epoch 49/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0565 - accuracy: 0.9755 - val_loss: 0.9878 - val_accuracy: 0.8469\n",
            "Epoch 50/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0547 - accuracy: 0.9760 - val_loss: 0.9782 - val_accuracy: 0.8475\n",
            "Epoch 51/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0541 - accuracy: 0.9762 - val_loss: 0.9955 - val_accuracy: 0.8470\n",
            "Epoch 52/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0537 - accuracy: 0.9762 - val_loss: 1.0125 - val_accuracy: 0.8478\n",
            "Epoch 53/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0537 - accuracy: 0.9763 - val_loss: 1.0221 - val_accuracy: 0.8476\n",
            "Epoch 54/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0529 - accuracy: 0.9767 - val_loss: 1.0007 - val_accuracy: 0.8469\n",
            "Epoch 55/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0534 - accuracy: 0.9763 - val_loss: 1.0127 - val_accuracy: 0.8492\n",
            "Epoch 56/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0532 - accuracy: 0.9763 - val_loss: 1.0360 - val_accuracy: 0.8467\n",
            "Epoch 57/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0530 - accuracy: 0.9764 - val_loss: 1.0110 - val_accuracy: 0.8470\n",
            "Epoch 58/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0526 - accuracy: 0.9765 - val_loss: 1.0540 - val_accuracy: 0.8449\n",
            "Epoch 59/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0529 - accuracy: 0.9763 - val_loss: 1.0347 - val_accuracy: 0.8459\n",
            "Epoch 60/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0514 - accuracy: 0.9770 - val_loss: 1.0641 - val_accuracy: 0.8445\n",
            "Epoch 61/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0493 - accuracy: 0.9773 - val_loss: 1.0931 - val_accuracy: 0.8459\n",
            "Epoch 62/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0482 - accuracy: 0.9779 - val_loss: 1.0755 - val_accuracy: 0.8478\n",
            "Epoch 63/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0492 - accuracy: 0.9773 - val_loss: 1.1019 - val_accuracy: 0.8453\n",
            "Epoch 64/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0489 - accuracy: 0.9776 - val_loss: 1.0811 - val_accuracy: 0.8461\n",
            "Epoch 65/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0485 - accuracy: 0.9778 - val_loss: 1.0837 - val_accuracy: 0.8469\n",
            "Epoch 66/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0480 - accuracy: 0.9780 - val_loss: 1.1103 - val_accuracy: 0.8453\n",
            "Epoch 67/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0476 - accuracy: 0.9781 - val_loss: 1.0989 - val_accuracy: 0.8474\n",
            "Epoch 68/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0477 - accuracy: 0.9779 - val_loss: 1.0977 - val_accuracy: 0.8486\n",
            "Epoch 69/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0485 - accuracy: 0.9776 - val_loss: 1.1090 - val_accuracy: 0.8451\n",
            "Epoch 70/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0483 - accuracy: 0.9780 - val_loss: 1.1244 - val_accuracy: 0.8458\n",
            "Epoch 71/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0475 - accuracy: 0.9778 - val_loss: 1.1282 - val_accuracy: 0.8466\n",
            "Epoch 72/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0462 - accuracy: 0.9784 - val_loss: 1.1252 - val_accuracy: 0.8446\n",
            "Epoch 73/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0470 - accuracy: 0.9781 - val_loss: 1.1453 - val_accuracy: 0.8450\n",
            "Epoch 74/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0470 - accuracy: 0.9779 - val_loss: 1.1432 - val_accuracy: 0.8467\n",
            "Epoch 75/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0460 - accuracy: 0.9784 - val_loss: 1.1597 - val_accuracy: 0.8469\n",
            "Epoch 76/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0463 - accuracy: 0.9784 - val_loss: 1.1773 - val_accuracy: 0.8456\n",
            "Epoch 77/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0447 - accuracy: 0.9789 - val_loss: 1.1492 - val_accuracy: 0.8456\n",
            "Epoch 78/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0436 - accuracy: 0.9790 - val_loss: 1.1860 - val_accuracy: 0.8461\n",
            "Epoch 79/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0429 - accuracy: 0.9791 - val_loss: 1.1747 - val_accuracy: 0.8469\n",
            "Epoch 80/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0427 - accuracy: 0.9792 - val_loss: 1.2207 - val_accuracy: 0.8467\n",
            "Epoch 81/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0435 - accuracy: 0.9791 - val_loss: 1.1848 - val_accuracy: 0.8467\n",
            "Epoch 82/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0445 - accuracy: 0.9787 - val_loss: 1.2093 - val_accuracy: 0.8453\n",
            "Epoch 83/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0479 - accuracy: 0.9776 - val_loss: 1.1892 - val_accuracy: 0.8448\n",
            "Epoch 84/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0471 - accuracy: 0.9780 - val_loss: 1.2226 - val_accuracy: 0.8451\n",
            "Epoch 85/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0461 - accuracy: 0.9786 - val_loss: 1.1891 - val_accuracy: 0.8467\n",
            "Epoch 86/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0435 - accuracy: 0.9791 - val_loss: 1.1930 - val_accuracy: 0.8461\n",
            "Epoch 87/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0419 - accuracy: 0.9795 - val_loss: 1.2276 - val_accuracy: 0.8476\n",
            "Epoch 88/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0418 - accuracy: 0.9798 - val_loss: 1.2341 - val_accuracy: 0.8466\n",
            "Epoch 89/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0416 - accuracy: 0.9796 - val_loss: 1.2012 - val_accuracy: 0.8451\n",
            "Epoch 90/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.2420 - val_accuracy: 0.8455\n",
            "Epoch 91/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0407 - accuracy: 0.9798 - val_loss: 1.2258 - val_accuracy: 0.8473\n",
            "Epoch 92/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0410 - accuracy: 0.9798 - val_loss: 1.2886 - val_accuracy: 0.8467\n",
            "Epoch 93/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0435 - accuracy: 0.9790 - val_loss: 1.1627 - val_accuracy: 0.8438\n",
            "Epoch 94/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0476 - accuracy: 0.9777 - val_loss: 1.2146 - val_accuracy: 0.8459\n",
            "Epoch 95/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0446 - accuracy: 0.9789 - val_loss: 1.2049 - val_accuracy: 0.8475\n",
            "Epoch 96/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0415 - accuracy: 0.9799 - val_loss: 1.2484 - val_accuracy: 0.8471\n",
            "Epoch 97/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0406 - accuracy: 0.9800 - val_loss: 1.2869 - val_accuracy: 0.8469\n",
            "Epoch 98/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0398 - accuracy: 0.9800 - val_loss: 1.2808 - val_accuracy: 0.8473\n",
            "Epoch 99/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0391 - accuracy: 0.9801 - val_loss: 1.3006 - val_accuracy: 0.8467\n",
            "Epoch 100/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0389 - accuracy: 0.9801 - val_loss: 1.3134 - val_accuracy: 0.8481\n",
            "Epoch 101/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0389 - accuracy: 0.9802 - val_loss: 1.3182 - val_accuracy: 0.8462\n",
            "Epoch 102/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0389 - accuracy: 0.9800 - val_loss: 1.2950 - val_accuracy: 0.8468\n",
            "Epoch 103/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0397 - accuracy: 0.9799 - val_loss: 1.2809 - val_accuracy: 0.8457\n",
            "Epoch 104/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0451 - accuracy: 0.9784 - val_loss: 1.2300 - val_accuracy: 0.8458\n",
            "Epoch 105/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0499 - accuracy: 0.9770 - val_loss: 1.2226 - val_accuracy: 0.8463\n",
            "Epoch 106/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0443 - accuracy: 0.9792 - val_loss: 1.2391 - val_accuracy: 0.8470\n",
            "Epoch 107/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0408 - accuracy: 0.9802 - val_loss: 1.2914 - val_accuracy: 0.8461\n",
            "Epoch 108/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0393 - accuracy: 0.9805 - val_loss: 1.3058 - val_accuracy: 0.8467\n",
            "Epoch 109/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0378 - accuracy: 0.9809 - val_loss: 1.3266 - val_accuracy: 0.8469\n",
            "Epoch 110/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0379 - accuracy: 0.9807 - val_loss: 1.3489 - val_accuracy: 0.8479\n",
            "Epoch 111/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0376 - accuracy: 0.9805 - val_loss: 1.3408 - val_accuracy: 0.8478\n",
            "Epoch 112/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0375 - accuracy: 0.9806 - val_loss: 1.3483 - val_accuracy: 0.8486\n",
            "Epoch 113/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0377 - accuracy: 0.9805 - val_loss: 1.3297 - val_accuracy: 0.8464\n",
            "Epoch 114/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0418 - accuracy: 0.9795 - val_loss: 1.3197 - val_accuracy: 0.8459\n",
            "Epoch 115/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0460 - accuracy: 0.9782 - val_loss: 1.2134 - val_accuracy: 0.8446\n",
            "Epoch 116/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0429 - accuracy: 0.9793 - val_loss: 1.2753 - val_accuracy: 0.8473\n",
            "Epoch 117/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0394 - accuracy: 0.9805 - val_loss: 1.2887 - val_accuracy: 0.8454\n",
            "Epoch 118/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0381 - accuracy: 0.9809 - val_loss: 1.3482 - val_accuracy: 0.8454\n",
            "Epoch 119/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0376 - accuracy: 0.9810 - val_loss: 1.3408 - val_accuracy: 0.8455\n",
            "Epoch 120/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0371 - accuracy: 0.9812 - val_loss: 1.3741 - val_accuracy: 0.8452\n",
            "Epoch 121/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0368 - accuracy: 0.9808 - val_loss: 1.3463 - val_accuracy: 0.8452\n",
            "Epoch 122/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0369 - accuracy: 0.9808 - val_loss: 1.3594 - val_accuracy: 0.8470\n",
            "Epoch 123/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0372 - accuracy: 0.9808 - val_loss: 1.3488 - val_accuracy: 0.8438\n",
            "Epoch 124/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0370 - accuracy: 0.9808 - val_loss: 1.3936 - val_accuracy: 0.8457\n",
            "Epoch 125/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0369 - accuracy: 0.9807 - val_loss: 1.4016 - val_accuracy: 0.8456\n",
            "Epoch 126/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0377 - accuracy: 0.9804 - val_loss: 1.4272 - val_accuracy: 0.8440\n",
            "Epoch 127/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0445 - accuracy: 0.9788 - val_loss: 1.3037 - val_accuracy: 0.8454\n",
            "Epoch 128/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0509 - accuracy: 0.9768 - val_loss: 1.2652 - val_accuracy: 0.8453\n",
            "Epoch 129/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0427 - accuracy: 0.9797 - val_loss: 1.3124 - val_accuracy: 0.8454\n",
            "Epoch 130/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0379 - accuracy: 0.9811 - val_loss: 1.3385 - val_accuracy: 0.8462\n",
            "Epoch 131/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0361 - accuracy: 0.9815 - val_loss: 1.3755 - val_accuracy: 0.8470\n",
            "Epoch 132/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0355 - accuracy: 0.9814 - val_loss: 1.3992 - val_accuracy: 0.8470\n",
            "Epoch 133/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0354 - accuracy: 0.9814 - val_loss: 1.3992 - val_accuracy: 0.8468\n",
            "Epoch 134/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0355 - accuracy: 0.9814 - val_loss: 1.4153 - val_accuracy: 0.8466\n",
            "Epoch 135/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0357 - accuracy: 0.9811 - val_loss: 1.4202 - val_accuracy: 0.8472\n",
            "Epoch 136/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0359 - accuracy: 0.9811 - val_loss: 1.4202 - val_accuracy: 0.8465\n",
            "Epoch 137/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0363 - accuracy: 0.9809 - val_loss: 1.4087 - val_accuracy: 0.8460\n",
            "Epoch 138/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0372 - accuracy: 0.9805 - val_loss: 1.4061 - val_accuracy: 0.8463\n",
            "Epoch 139/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0429 - accuracy: 0.9791 - val_loss: 1.3369 - val_accuracy: 0.8446\n",
            "Epoch 140/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0435 - accuracy: 0.9794 - val_loss: 1.3249 - val_accuracy: 0.8461\n",
            "Epoch 141/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0390 - accuracy: 0.9806 - val_loss: 1.3594 - val_accuracy: 0.8445\n",
            "Epoch 142/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0370 - accuracy: 0.9812 - val_loss: 1.3639 - val_accuracy: 0.8473\n",
            "Epoch 143/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0361 - accuracy: 0.9813 - val_loss: 1.4215 - val_accuracy: 0.8460\n",
            "Epoch 144/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0361 - accuracy: 0.9815 - val_loss: 1.3866 - val_accuracy: 0.8450\n",
            "Epoch 145/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0356 - accuracy: 0.9814 - val_loss: 1.4167 - val_accuracy: 0.8458\n",
            "Epoch 146/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0354 - accuracy: 0.9813 - val_loss: 1.4173 - val_accuracy: 0.8448\n",
            "Epoch 147/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0352 - accuracy: 0.9812 - val_loss: 1.4636 - val_accuracy: 0.8454\n",
            "Epoch 148/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0353 - accuracy: 0.9813 - val_loss: 1.4046 - val_accuracy: 0.8451\n",
            "Epoch 149/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0357 - accuracy: 0.9810 - val_loss: 1.4297 - val_accuracy: 0.8447\n",
            "Epoch 150/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0358 - accuracy: 0.9811 - val_loss: 1.4373 - val_accuracy: 0.8458\n",
            "Epoch 151/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0372 - accuracy: 0.9807 - val_loss: 1.3559 - val_accuracy: 0.8436\n",
            "Epoch 152/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0425 - accuracy: 0.9793 - val_loss: 1.3466 - val_accuracy: 0.8427\n",
            "Epoch 153/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0435 - accuracy: 0.9793 - val_loss: 1.3273 - val_accuracy: 0.8445\n",
            "Epoch 154/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0395 - accuracy: 0.9804 - val_loss: 1.3788 - val_accuracy: 0.8458\n",
            "Epoch 155/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0364 - accuracy: 0.9815 - val_loss: 1.4211 - val_accuracy: 0.8460\n",
            "Epoch 156/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0351 - accuracy: 0.9818 - val_loss: 1.4245 - val_accuracy: 0.8462\n",
            "Epoch 157/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0345 - accuracy: 0.9818 - val_loss: 1.4599 - val_accuracy: 0.8463\n",
            "Epoch 158/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0343 - accuracy: 0.9817 - val_loss: 1.4780 - val_accuracy: 0.8471\n",
            "Epoch 159/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0343 - accuracy: 0.9817 - val_loss: 1.4720 - val_accuracy: 0.8466\n",
            "Epoch 160/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0344 - accuracy: 0.9814 - val_loss: 1.4968 - val_accuracy: 0.8473\n",
            "Epoch 161/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0346 - accuracy: 0.9815 - val_loss: 1.5253 - val_accuracy: 0.8476\n",
            "Epoch 162/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0346 - accuracy: 0.9814 - val_loss: 1.4792 - val_accuracy: 0.8457\n",
            "Epoch 163/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0350 - accuracy: 0.9813 - val_loss: 1.4706 - val_accuracy: 0.8464\n",
            "Epoch 164/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0388 - accuracy: 0.9804 - val_loss: 1.4017 - val_accuracy: 0.8427\n",
            "Epoch 165/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0465 - accuracy: 0.9782 - val_loss: 1.3766 - val_accuracy: 0.8437\n",
            "Epoch 166/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0423 - accuracy: 0.9799 - val_loss: 1.4067 - val_accuracy: 0.8456\n",
            "Epoch 167/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0368 - accuracy: 0.9815 - val_loss: 1.4201 - val_accuracy: 0.8456\n",
            "Epoch 168/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0347 - accuracy: 0.9819 - val_loss: 1.4520 - val_accuracy: 0.8463\n",
            "Epoch 169/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0340 - accuracy: 0.9823 - val_loss: 1.4524 - val_accuracy: 0.8466\n",
            "Epoch 170/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0337 - accuracy: 0.9822 - val_loss: 1.4626 - val_accuracy: 0.8453\n",
            "Epoch 171/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0337 - accuracy: 0.9820 - val_loss: 1.4765 - val_accuracy: 0.8457\n",
            "Epoch 172/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0338 - accuracy: 0.9821 - val_loss: 1.5094 - val_accuracy: 0.8465\n",
            "Epoch 173/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0339 - accuracy: 0.9818 - val_loss: 1.5128 - val_accuracy: 0.8472\n",
            "Epoch 174/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0340 - accuracy: 0.9817 - val_loss: 1.5350 - val_accuracy: 0.8466\n",
            "Epoch 175/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0341 - accuracy: 0.9816 - val_loss: 1.4866 - val_accuracy: 0.8465\n",
            "Epoch 176/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0345 - accuracy: 0.9817 - val_loss: 1.5263 - val_accuracy: 0.8448\n",
            "Epoch 177/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0359 - accuracy: 0.9813 - val_loss: 1.4639 - val_accuracy: 0.8446\n",
            "Epoch 178/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0407 - accuracy: 0.9799 - val_loss: 1.3985 - val_accuracy: 0.8448\n",
            "Epoch 179/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0441 - accuracy: 0.9790 - val_loss: 1.3813 - val_accuracy: 0.8455\n",
            "Epoch 180/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0401 - accuracy: 0.9805 - val_loss: 1.4078 - val_accuracy: 0.8454\n",
            "Epoch 181/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0358 - accuracy: 0.9818 - val_loss: 1.4401 - val_accuracy: 0.8461\n",
            "Epoch 182/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0340 - accuracy: 0.9822 - val_loss: 1.4720 - val_accuracy: 0.8479\n",
            "Epoch 183/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0336 - accuracy: 0.9822 - val_loss: 1.4974 - val_accuracy: 0.8474\n",
            "Epoch 184/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0333 - accuracy: 0.9822 - val_loss: 1.5151 - val_accuracy: 0.8473\n",
            "Epoch 185/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0333 - accuracy: 0.9820 - val_loss: 1.5243 - val_accuracy: 0.8463\n",
            "Epoch 186/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0334 - accuracy: 0.9819 - val_loss: 1.5222 - val_accuracy: 0.8473\n",
            "Epoch 187/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0336 - accuracy: 0.9820 - val_loss: 1.5222 - val_accuracy: 0.8464\n",
            "Epoch 188/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0336 - accuracy: 0.9818 - val_loss: 1.5398 - val_accuracy: 0.8467\n",
            "Epoch 189/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0337 - accuracy: 0.9819 - val_loss: 1.5602 - val_accuracy: 0.8468\n",
            "Epoch 190/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0337 - accuracy: 0.9819 - val_loss: 1.5537 - val_accuracy: 0.8458\n",
            "Epoch 191/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0338 - accuracy: 0.9819 - val_loss: 1.5391 - val_accuracy: 0.8456\n",
            "Epoch 192/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0366 - accuracy: 0.9812 - val_loss: 1.4313 - val_accuracy: 0.8433\n",
            "Epoch 193/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0466 - accuracy: 0.9782 - val_loss: 1.3951 - val_accuracy: 0.8461\n",
            "Epoch 194/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0416 - accuracy: 0.9802 - val_loss: 1.4089 - val_accuracy: 0.8452\n",
            "Epoch 195/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0358 - accuracy: 0.9819 - val_loss: 1.4812 - val_accuracy: 0.8468\n",
            "Epoch 196/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0336 - accuracy: 0.9824 - val_loss: 1.4876 - val_accuracy: 0.8455\n",
            "Epoch 197/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0331 - accuracy: 0.9824 - val_loss: 1.5085 - val_accuracy: 0.8464\n",
            "Epoch 198/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9825 - val_loss: 1.5248 - val_accuracy: 0.8457\n",
            "Epoch 199/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0329 - accuracy: 0.9823 - val_loss: 1.5355 - val_accuracy: 0.8463\n",
            "Epoch 200/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9823 - val_loss: 1.5317 - val_accuracy: 0.8461\n",
            "Epoch 201/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0330 - accuracy: 0.9821 - val_loss: 1.5448 - val_accuracy: 0.8468\n",
            "Epoch 202/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0330 - accuracy: 0.9821 - val_loss: 1.5734 - val_accuracy: 0.8467\n",
            "Epoch 203/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0331 - accuracy: 0.9820 - val_loss: 1.5707 - val_accuracy: 0.8465\n",
            "Epoch 204/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0332 - accuracy: 0.9820 - val_loss: 1.5992 - val_accuracy: 0.8461\n",
            "Epoch 205/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0332 - accuracy: 0.9818 - val_loss: 1.5910 - val_accuracy: 0.8470\n",
            "Epoch 206/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0333 - accuracy: 0.9819 - val_loss: 1.5827 - val_accuracy: 0.8473\n",
            "Epoch 207/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0335 - accuracy: 0.9819 - val_loss: 1.5688 - val_accuracy: 0.8462\n",
            "Epoch 208/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0361 - accuracy: 0.9812 - val_loss: 1.4882 - val_accuracy: 0.8422\n",
            "Epoch 209/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0513 - accuracy: 0.9770 - val_loss: 1.4062 - val_accuracy: 0.8452\n",
            "Epoch 210/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0400 - accuracy: 0.9807 - val_loss: 1.4289 - val_accuracy: 0.8462\n",
            "Epoch 211/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0350 - accuracy: 0.9824 - val_loss: 1.4782 - val_accuracy: 0.8458\n",
            "Epoch 212/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0330 - accuracy: 0.9828 - val_loss: 1.5099 - val_accuracy: 0.8466\n",
            "Epoch 213/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0324 - accuracy: 0.9828 - val_loss: 1.5450 - val_accuracy: 0.8456\n",
            "Epoch 214/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0323 - accuracy: 0.9828 - val_loss: 1.5550 - val_accuracy: 0.8464\n",
            "Epoch 215/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0323 - accuracy: 0.9826 - val_loss: 1.5617 - val_accuracy: 0.8465\n",
            "Epoch 216/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0324 - accuracy: 0.9824 - val_loss: 1.5810 - val_accuracy: 0.8468\n",
            "Epoch 217/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0326 - accuracy: 0.9824 - val_loss: 1.5980 - val_accuracy: 0.8466\n",
            "Epoch 218/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0326 - accuracy: 0.9823 - val_loss: 1.6029 - val_accuracy: 0.8461\n",
            "Epoch 219/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0327 - accuracy: 0.9824 - val_loss: 1.6016 - val_accuracy: 0.8468\n",
            "Epoch 220/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9823 - val_loss: 1.5906 - val_accuracy: 0.8457\n",
            "Epoch 221/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0339 - accuracy: 0.9820 - val_loss: 1.5552 - val_accuracy: 0.8443\n",
            "Epoch 222/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0414 - accuracy: 0.9798 - val_loss: 1.4723 - val_accuracy: 0.8453\n",
            "Epoch 223/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0417 - accuracy: 0.9801 - val_loss: 1.4807 - val_accuracy: 0.8462\n",
            "Epoch 224/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0368 - accuracy: 0.9817 - val_loss: 1.4603 - val_accuracy: 0.8450\n",
            "Epoch 225/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0335 - accuracy: 0.9827 - val_loss: 1.5137 - val_accuracy: 0.8473\n",
            "Epoch 226/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0326 - accuracy: 0.9830 - val_loss: 1.5368 - val_accuracy: 0.8476\n",
            "Epoch 227/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0322 - accuracy: 0.9827 - val_loss: 1.5644 - val_accuracy: 0.8470\n",
            "Epoch 228/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0322 - accuracy: 0.9826 - val_loss: 1.5728 - val_accuracy: 0.8464\n",
            "Epoch 229/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0322 - accuracy: 0.9826 - val_loss: 1.5972 - val_accuracy: 0.8482\n",
            "Epoch 230/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0322 - accuracy: 0.9827 - val_loss: 1.5992 - val_accuracy: 0.8464\n",
            "Epoch 231/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0323 - accuracy: 0.9825 - val_loss: 1.6116 - val_accuracy: 0.8473\n",
            "Epoch 232/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0324 - accuracy: 0.9824 - val_loss: 1.6280 - val_accuracy: 0.8463\n",
            "Epoch 233/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0327 - accuracy: 0.9824 - val_loss: 1.5509 - val_accuracy: 0.8466\n",
            "Epoch 234/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0351 - accuracy: 0.9817 - val_loss: 1.5096 - val_accuracy: 0.8451\n",
            "Epoch 235/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0431 - accuracy: 0.9794 - val_loss: 1.4680 - val_accuracy: 0.8454\n",
            "Epoch 236/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0397 - accuracy: 0.9810 - val_loss: 1.4234 - val_accuracy: 0.8453\n",
            "Epoch 237/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0352 - accuracy: 0.9822 - val_loss: 1.5076 - val_accuracy: 0.8453\n",
            "Epoch 238/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0330 - accuracy: 0.9829 - val_loss: 1.5230 - val_accuracy: 0.8463\n",
            "Epoch 239/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0323 - accuracy: 0.9828 - val_loss: 1.5732 - val_accuracy: 0.8475\n",
            "Epoch 240/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0321 - accuracy: 0.9829 - val_loss: 1.5904 - val_accuracy: 0.8470\n",
            "Epoch 241/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0320 - accuracy: 0.9828 - val_loss: 1.6027 - val_accuracy: 0.8466\n",
            "Epoch 242/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0320 - accuracy: 0.9826 - val_loss: 1.6078 - val_accuracy: 0.8470\n",
            "Epoch 243/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0321 - accuracy: 0.9826 - val_loss: 1.6085 - val_accuracy: 0.8465\n",
            "Epoch 244/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0321 - accuracy: 0.9826 - val_loss: 1.6271 - val_accuracy: 0.8467\n",
            "Epoch 245/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0322 - accuracy: 0.9823 - val_loss: 1.6324 - val_accuracy: 0.8460\n",
            "Epoch 246/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0325 - accuracy: 0.9825 - val_loss: 1.6258 - val_accuracy: 0.8482\n",
            "Epoch 247/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0332 - accuracy: 0.9824 - val_loss: 1.5880 - val_accuracy: 0.8456\n",
            "Epoch 248/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0366 - accuracy: 0.9814 - val_loss: 1.5621 - val_accuracy: 0.8469\n",
            "Epoch 249/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0412 - accuracy: 0.9802 - val_loss: 1.4725 - val_accuracy: 0.8446\n",
            "Epoch 250/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0380 - accuracy: 0.9813 - val_loss: 1.5151 - val_accuracy: 0.8444\n",
            "Epoch 251/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0344 - accuracy: 0.9825 - val_loss: 1.5308 - val_accuracy: 0.8460\n",
            "Epoch 252/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0326 - accuracy: 0.9829 - val_loss: 1.5744 - val_accuracy: 0.8460\n",
            "Epoch 253/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0320 - accuracy: 0.9829 - val_loss: 1.5808 - val_accuracy: 0.8463\n",
            "Epoch 254/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9830 - val_loss: 1.6254 - val_accuracy: 0.8462\n",
            "Epoch 255/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0317 - accuracy: 0.9830 - val_loss: 1.6394 - val_accuracy: 0.8462\n",
            "Epoch 256/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9827 - val_loss: 1.6353 - val_accuracy: 0.8466\n",
            "Epoch 257/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9828 - val_loss: 1.6418 - val_accuracy: 0.8460\n",
            "Epoch 258/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0319 - accuracy: 0.9826 - val_loss: 1.6398 - val_accuracy: 0.8466\n",
            "Epoch 259/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0320 - accuracy: 0.9826 - val_loss: 1.6694 - val_accuracy: 0.8468\n",
            "Epoch 260/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0320 - accuracy: 0.9826 - val_loss: 1.6546 - val_accuracy: 0.8464\n",
            "Epoch 261/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0322 - accuracy: 0.9824 - val_loss: 1.6534 - val_accuracy: 0.8462\n",
            "Epoch 262/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0326 - accuracy: 0.9824 - val_loss: 1.6607 - val_accuracy: 0.8455\n",
            "Epoch 263/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0337 - accuracy: 0.9819 - val_loss: 1.6039 - val_accuracy: 0.8443\n",
            "Epoch 264/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0420 - accuracy: 0.9798 - val_loss: 1.5710 - val_accuracy: 0.8435\n",
            "Epoch 265/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0431 - accuracy: 0.9801 - val_loss: 1.4931 - val_accuracy: 0.8426\n",
            "Epoch 266/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0374 - accuracy: 0.9817 - val_loss: 1.5021 - val_accuracy: 0.8436\n",
            "Epoch 267/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0335 - accuracy: 0.9828 - val_loss: 1.5590 - val_accuracy: 0.8456\n",
            "Epoch 268/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0324 - accuracy: 0.9831 - val_loss: 1.5734 - val_accuracy: 0.8443\n",
            "Epoch 269/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0321 - accuracy: 0.9831 - val_loss: 1.5637 - val_accuracy: 0.8451\n",
            "Epoch 270/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9831 - val_loss: 1.6003 - val_accuracy: 0.8462\n",
            "Epoch 271/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0314 - accuracy: 0.9832 - val_loss: 1.6253 - val_accuracy: 0.8463\n",
            "Epoch 272/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0314 - accuracy: 0.9830 - val_loss: 1.6384 - val_accuracy: 0.8466\n",
            "Epoch 273/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0315 - accuracy: 0.9829 - val_loss: 1.6384 - val_accuracy: 0.8473\n",
            "Epoch 274/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9828 - val_loss: 1.6503 - val_accuracy: 0.8466\n",
            "Epoch 275/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0317 - accuracy: 0.9827 - val_loss: 1.6590 - val_accuracy: 0.8471\n",
            "Epoch 276/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0317 - accuracy: 0.9827 - val_loss: 1.6817 - val_accuracy: 0.8465\n",
            "Epoch 277/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9827 - val_loss: 1.6613 - val_accuracy: 0.8465\n",
            "Epoch 278/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0319 - accuracy: 0.9824 - val_loss: 1.6607 - val_accuracy: 0.8459\n",
            "Epoch 279/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0350 - accuracy: 0.9818 - val_loss: 1.5743 - val_accuracy: 0.8428\n",
            "Epoch 280/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0425 - accuracy: 0.9798 - val_loss: 1.5262 - val_accuracy: 0.8441\n",
            "Epoch 281/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0385 - accuracy: 0.9813 - val_loss: 1.5524 - val_accuracy: 0.8461\n",
            "Epoch 282/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0340 - accuracy: 0.9827 - val_loss: 1.5982 - val_accuracy: 0.8460\n",
            "Epoch 283/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0322 - accuracy: 0.9831 - val_loss: 1.6099 - val_accuracy: 0.8465\n",
            "Epoch 284/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9832 - val_loss: 1.6360 - val_accuracy: 0.8459\n",
            "Epoch 285/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9833 - val_loss: 1.6478 - val_accuracy: 0.8456\n",
            "Epoch 286/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9831 - val_loss: 1.6709 - val_accuracy: 0.8458\n",
            "Epoch 287/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9831 - val_loss: 1.6733 - val_accuracy: 0.8456\n",
            "Epoch 288/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9830 - val_loss: 1.6921 - val_accuracy: 0.8456\n",
            "Epoch 289/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0314 - accuracy: 0.9831 - val_loss: 1.6939 - val_accuracy: 0.8465\n",
            "Epoch 290/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9828 - val_loss: 1.7069 - val_accuracy: 0.8458\n",
            "Epoch 291/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0316 - accuracy: 0.9827 - val_loss: 1.7041 - val_accuracy: 0.8457\n",
            "Epoch 292/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0317 - accuracy: 0.9829 - val_loss: 1.6799 - val_accuracy: 0.8464\n",
            "Epoch 293/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0364 - accuracy: 0.9815 - val_loss: 1.5830 - val_accuracy: 0.8442\n",
            "Epoch 294/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0417 - accuracy: 0.9802 - val_loss: 1.5347 - val_accuracy: 0.8454\n",
            "Epoch 295/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0364 - accuracy: 0.9818 - val_loss: 1.6047 - val_accuracy: 0.8460\n",
            "Epoch 296/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0341 - accuracy: 0.9828 - val_loss: 1.5910 - val_accuracy: 0.8454\n",
            "Epoch 297/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0323 - accuracy: 0.9832 - val_loss: 1.6096 - val_accuracy: 0.8441\n",
            "Epoch 298/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9833 - val_loss: 1.6371 - val_accuracy: 0.8452\n",
            "Epoch 299/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9834 - val_loss: 1.6576 - val_accuracy: 0.8458\n",
            "Epoch 300/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9834 - val_loss: 1.6731 - val_accuracy: 0.8450\n",
            "Epoch 301/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9831 - val_loss: 1.6622 - val_accuracy: 0.8451\n",
            "Epoch 302/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9830 - val_loss: 1.6967 - val_accuracy: 0.8458\n",
            "Epoch 303/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9830 - val_loss: 1.6959 - val_accuracy: 0.8453\n",
            "Epoch 304/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9829 - val_loss: 1.7310 - val_accuracy: 0.8451\n",
            "Epoch 305/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0314 - accuracy: 0.9828 - val_loss: 1.7128 - val_accuracy: 0.8446\n",
            "Epoch 306/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9828 - val_loss: 1.7364 - val_accuracy: 0.8454\n",
            "Epoch 307/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9828 - val_loss: 1.7091 - val_accuracy: 0.8448\n",
            "Epoch 308/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0316 - accuracy: 0.9828 - val_loss: 1.7207 - val_accuracy: 0.8444\n",
            "Epoch 309/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0320 - accuracy: 0.9826 - val_loss: 1.7125 - val_accuracy: 0.8447\n",
            "Epoch 310/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0398 - accuracy: 0.9805 - val_loss: 1.5947 - val_accuracy: 0.8446\n",
            "Epoch 311/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0420 - accuracy: 0.9800 - val_loss: 1.5627 - val_accuracy: 0.8458\n",
            "Epoch 312/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0359 - accuracy: 0.9822 - val_loss: 1.5789 - val_accuracy: 0.8465\n",
            "Epoch 313/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0328 - accuracy: 0.9834 - val_loss: 1.6375 - val_accuracy: 0.8454\n",
            "Epoch 314/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9835 - val_loss: 1.6513 - val_accuracy: 0.8463\n",
            "Epoch 315/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0310 - accuracy: 0.9835 - val_loss: 1.6762 - val_accuracy: 0.8455\n",
            "Epoch 316/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9834 - val_loss: 1.6823 - val_accuracy: 0.8456\n",
            "Epoch 317/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9832 - val_loss: 1.7075 - val_accuracy: 0.8456\n",
            "Epoch 318/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9832 - val_loss: 1.6805 - val_accuracy: 0.8462\n",
            "Epoch 319/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0320 - accuracy: 0.9831 - val_loss: 1.6537 - val_accuracy: 0.8461\n",
            "Epoch 320/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9828 - val_loss: 1.6729 - val_accuracy: 0.8463\n",
            "Epoch 321/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9829 - val_loss: 1.7005 - val_accuracy: 0.8462\n",
            "Epoch 322/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0328 - accuracy: 0.9829 - val_loss: 1.6683 - val_accuracy: 0.8457\n",
            "Epoch 323/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0325 - accuracy: 0.9830 - val_loss: 1.6741 - val_accuracy: 0.8461\n",
            "Epoch 324/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0323 - accuracy: 0.9829 - val_loss: 1.6883 - val_accuracy: 0.8454\n",
            "Epoch 325/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0323 - accuracy: 0.9830 - val_loss: 1.6565 - val_accuracy: 0.8475\n",
            "Epoch 326/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9831 - val_loss: 1.7079 - val_accuracy: 0.8473\n",
            "Epoch 327/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0316 - accuracy: 0.9832 - val_loss: 1.6972 - val_accuracy: 0.8460\n",
            "Epoch 328/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9832 - val_loss: 1.7235 - val_accuracy: 0.8455\n",
            "Epoch 329/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9833 - val_loss: 1.7497 - val_accuracy: 0.8468\n",
            "Epoch 330/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0313 - accuracy: 0.9830 - val_loss: 1.7666 - val_accuracy: 0.8467\n",
            "Epoch 331/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9831 - val_loss: 1.7350 - val_accuracy: 0.8475\n",
            "Epoch 332/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9832 - val_loss: 1.7644 - val_accuracy: 0.8460\n",
            "Epoch 333/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9830 - val_loss: 1.7394 - val_accuracy: 0.8465\n",
            "Epoch 334/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9830 - val_loss: 1.7538 - val_accuracy: 0.8472\n",
            "Epoch 335/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0316 - accuracy: 0.9828 - val_loss: 1.7672 - val_accuracy: 0.8458\n",
            "Epoch 336/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0346 - accuracy: 0.9820 - val_loss: 1.6772 - val_accuracy: 0.8459\n",
            "Epoch 337/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0443 - accuracy: 0.9793 - val_loss: 1.5911 - val_accuracy: 0.8450\n",
            "Epoch 338/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0382 - accuracy: 0.9815 - val_loss: 1.6264 - val_accuracy: 0.8444\n",
            "Epoch 339/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0329 - accuracy: 0.9832 - val_loss: 1.6518 - val_accuracy: 0.8459\n",
            "Epoch 340/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0312 - accuracy: 0.9836 - val_loss: 1.6875 - val_accuracy: 0.8459\n",
            "Epoch 341/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9837 - val_loss: 1.6911 - val_accuracy: 0.8459\n",
            "Epoch 342/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9837 - val_loss: 1.7144 - val_accuracy: 0.8461\n",
            "Epoch 343/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9836 - val_loss: 1.7369 - val_accuracy: 0.8454\n",
            "Epoch 344/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9834 - val_loss: 1.7398 - val_accuracy: 0.8463\n",
            "Epoch 345/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 1.7553 - val_accuracy: 0.8461\n",
            "Epoch 346/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9834 - val_loss: 1.7541 - val_accuracy: 0.8461\n",
            "Epoch 347/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0308 - accuracy: 0.9832 - val_loss: 1.7651 - val_accuracy: 0.8470\n",
            "Epoch 348/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9830 - val_loss: 1.7866 - val_accuracy: 0.8457\n",
            "Epoch 349/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9830 - val_loss: 1.7794 - val_accuracy: 0.8461\n",
            "Epoch 350/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9829 - val_loss: 1.7951 - val_accuracy: 0.8457\n",
            "Epoch 351/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0310 - accuracy: 0.9831 - val_loss: 1.7893 - val_accuracy: 0.8469\n",
            "Epoch 352/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0310 - accuracy: 0.9829 - val_loss: 1.7905 - val_accuracy: 0.8470\n",
            "Epoch 353/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9830 - val_loss: 1.8061 - val_accuracy: 0.8456\n",
            "Epoch 354/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0344 - accuracy: 0.9821 - val_loss: 1.7048 - val_accuracy: 0.8456\n",
            "Epoch 355/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0488 - accuracy: 0.9781 - val_loss: 1.5850 - val_accuracy: 0.8442\n",
            "Epoch 356/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0391 - accuracy: 0.9814 - val_loss: 1.5949 - val_accuracy: 0.8460\n",
            "Epoch 357/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0333 - accuracy: 0.9831 - val_loss: 1.6350 - val_accuracy: 0.8474\n",
            "Epoch 358/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0315 - accuracy: 0.9836 - val_loss: 1.6655 - val_accuracy: 0.8472\n",
            "Epoch 359/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9838 - val_loss: 1.6871 - val_accuracy: 0.8470\n",
            "Epoch 360/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9837 - val_loss: 1.6965 - val_accuracy: 0.8472\n",
            "Epoch 361/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9839 - val_loss: 1.7356 - val_accuracy: 0.8463\n",
            "Epoch 362/500\n",
            "282/282 [==============================] - 10s 34ms/step - loss: 0.0305 - accuracy: 0.9835 - val_loss: 1.7332 - val_accuracy: 0.8467\n",
            "Epoch 363/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9835 - val_loss: 1.7506 - val_accuracy: 0.8474\n",
            "Epoch 364/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9836 - val_loss: 1.7488 - val_accuracy: 0.8462\n",
            "Epoch 365/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0310 - accuracy: 0.9833 - val_loss: 1.7551 - val_accuracy: 0.8460\n",
            "Epoch 366/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0316 - accuracy: 0.9830 - val_loss: 1.7052 - val_accuracy: 0.8461\n",
            "Epoch 367/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0337 - accuracy: 0.9827 - val_loss: 1.6885 - val_accuracy: 0.8455\n",
            "Epoch 368/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0387 - accuracy: 0.9815 - val_loss: 1.6403 - val_accuracy: 0.8452\n",
            "Epoch 369/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0343 - accuracy: 0.9827 - val_loss: 1.6503 - val_accuracy: 0.8456\n",
            "Epoch 370/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0319 - accuracy: 0.9836 - val_loss: 1.6777 - val_accuracy: 0.8446\n",
            "Epoch 371/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0308 - accuracy: 0.9837 - val_loss: 1.7117 - val_accuracy: 0.8446\n",
            "Epoch 372/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9837 - val_loss: 1.7445 - val_accuracy: 0.8449\n",
            "Epoch 373/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9835 - val_loss: 1.7551 - val_accuracy: 0.8451\n",
            "Epoch 374/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9835 - val_loss: 1.7732 - val_accuracy: 0.8456\n",
            "Epoch 375/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9835 - val_loss: 1.7743 - val_accuracy: 0.8452\n",
            "Epoch 376/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9834 - val_loss: 1.7788 - val_accuracy: 0.8450\n",
            "Epoch 377/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9834 - val_loss: 1.7973 - val_accuracy: 0.8450\n",
            "Epoch 378/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9833 - val_loss: 1.7825 - val_accuracy: 0.8450\n",
            "Epoch 379/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9832 - val_loss: 1.8106 - val_accuracy: 0.8449\n",
            "Epoch 380/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0308 - accuracy: 0.9832 - val_loss: 1.8096 - val_accuracy: 0.8457\n",
            "Epoch 381/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0324 - accuracy: 0.9827 - val_loss: 1.7924 - val_accuracy: 0.8437\n",
            "Epoch 382/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0411 - accuracy: 0.9804 - val_loss: 1.5886 - val_accuracy: 0.8453\n",
            "Epoch 383/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0386 - accuracy: 0.9815 - val_loss: 1.6390 - val_accuracy: 0.8444\n",
            "Epoch 384/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0340 - accuracy: 0.9830 - val_loss: 1.6701 - val_accuracy: 0.8457\n",
            "Epoch 385/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9836 - val_loss: 1.7067 - val_accuracy: 0.8447\n",
            "Epoch 386/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9838 - val_loss: 1.7000 - val_accuracy: 0.8449\n",
            "Epoch 387/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9838 - val_loss: 1.7300 - val_accuracy: 0.8449\n",
            "Epoch 388/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9837 - val_loss: 1.7656 - val_accuracy: 0.8456\n",
            "Epoch 389/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0303 - accuracy: 0.9838 - val_loss: 1.7744 - val_accuracy: 0.8458\n",
            "Epoch 390/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9836 - val_loss: 1.7779 - val_accuracy: 0.8453\n",
            "Epoch 391/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9834 - val_loss: 1.7825 - val_accuracy: 0.8456\n",
            "Epoch 392/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9836 - val_loss: 1.7918 - val_accuracy: 0.8456\n",
            "Epoch 393/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0305 - accuracy: 0.9834 - val_loss: 1.8043 - val_accuracy: 0.8449\n",
            "Epoch 394/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 1.7754 - val_accuracy: 0.8451\n",
            "Epoch 395/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9833 - val_loss: 1.8021 - val_accuracy: 0.8454\n",
            "Epoch 396/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9832 - val_loss: 1.8069 - val_accuracy: 0.8454\n",
            "Epoch 397/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9831 - val_loss: 1.7925 - val_accuracy: 0.8457\n",
            "Epoch 398/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0308 - accuracy: 0.9833 - val_loss: 1.8184 - val_accuracy: 0.8448\n",
            "Epoch 399/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0310 - accuracy: 0.9830 - val_loss: 1.7975 - val_accuracy: 0.8446\n",
            "Epoch 400/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0333 - accuracy: 0.9824 - val_loss: 1.7410 - val_accuracy: 0.8443\n",
            "Epoch 401/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0441 - accuracy: 0.9794 - val_loss: 1.6784 - val_accuracy: 0.8441\n",
            "Epoch 402/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0377 - accuracy: 0.9818 - val_loss: 1.6483 - val_accuracy: 0.8433\n",
            "Epoch 403/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0330 - accuracy: 0.9834 - val_loss: 1.6950 - val_accuracy: 0.8458\n",
            "Epoch 404/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0311 - accuracy: 0.9840 - val_loss: 1.7002 - val_accuracy: 0.8455\n",
            "Epoch 405/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0305 - accuracy: 0.9840 - val_loss: 1.7373 - val_accuracy: 0.8456\n",
            "Epoch 406/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9839 - val_loss: 1.7762 - val_accuracy: 0.8463\n",
            "Epoch 407/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9839 - val_loss: 1.7763 - val_accuracy: 0.8458\n",
            "Epoch 408/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.7923 - val_accuracy: 0.8451\n",
            "Epoch 409/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0302 - accuracy: 0.9837 - val_loss: 1.8153 - val_accuracy: 0.8454\n",
            "Epoch 410/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9836 - val_loss: 1.8175 - val_accuracy: 0.8456\n",
            "Epoch 411/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0303 - accuracy: 0.9836 - val_loss: 1.8121 - val_accuracy: 0.8462\n",
            "Epoch 412/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9835 - val_loss: 1.8286 - val_accuracy: 0.8458\n",
            "Epoch 413/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0307 - accuracy: 0.9834 - val_loss: 1.8325 - val_accuracy: 0.8440\n",
            "Epoch 414/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0323 - accuracy: 0.9829 - val_loss: 1.7090 - val_accuracy: 0.8455\n",
            "Epoch 415/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0390 - accuracy: 0.9811 - val_loss: 1.7011 - val_accuracy: 0.8445\n",
            "Epoch 416/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0379 - accuracy: 0.9814 - val_loss: 1.6789 - val_accuracy: 0.8461\n",
            "Epoch 417/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0330 - accuracy: 0.9831 - val_loss: 1.6972 - val_accuracy: 0.8447\n",
            "Epoch 418/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0311 - accuracy: 0.9838 - val_loss: 1.7368 - val_accuracy: 0.8460\n",
            "Epoch 419/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0305 - accuracy: 0.9838 - val_loss: 1.7670 - val_accuracy: 0.8459\n",
            "Epoch 420/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.7948 - val_accuracy: 0.8452\n",
            "Epoch 421/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.8010 - val_accuracy: 0.8456\n",
            "Epoch 422/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0302 - accuracy: 0.9837 - val_loss: 1.8140 - val_accuracy: 0.8456\n",
            "Epoch 423/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.8146 - val_accuracy: 0.8456\n",
            "Epoch 424/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9835 - val_loss: 1.8341 - val_accuracy: 0.8455\n",
            "Epoch 425/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0303 - accuracy: 0.9835 - val_loss: 1.8324 - val_accuracy: 0.8457\n",
            "Epoch 426/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9834 - val_loss: 1.8516 - val_accuracy: 0.8452\n",
            "Epoch 427/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9835 - val_loss: 1.8413 - val_accuracy: 0.8457\n",
            "Epoch 428/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0305 - accuracy: 0.9834 - val_loss: 1.8441 - val_accuracy: 0.8448\n",
            "Epoch 429/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0306 - accuracy: 0.9834 - val_loss: 1.8547 - val_accuracy: 0.8446\n",
            "Epoch 430/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0309 - accuracy: 0.9832 - val_loss: 1.8098 - val_accuracy: 0.8447\n",
            "Epoch 431/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0383 - accuracy: 0.9813 - val_loss: 1.6869 - val_accuracy: 0.8445\n",
            "Epoch 432/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0416 - accuracy: 0.9806 - val_loss: 1.7100 - val_accuracy: 0.8445\n",
            "Epoch 433/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0351 - accuracy: 0.9825 - val_loss: 1.7047 - val_accuracy: 0.8444\n",
            "Epoch 434/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0321 - accuracy: 0.9835 - val_loss: 1.7074 - val_accuracy: 0.8445\n",
            "Epoch 435/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0308 - accuracy: 0.9839 - val_loss: 1.7288 - val_accuracy: 0.8452\n",
            "Epoch 436/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9837 - val_loss: 1.7703 - val_accuracy: 0.8459\n",
            "Epoch 437/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.7976 - val_accuracy: 0.8461\n",
            "Epoch 438/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9838 - val_loss: 1.7984 - val_accuracy: 0.8457\n",
            "Epoch 439/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9837 - val_loss: 1.8197 - val_accuracy: 0.8458\n",
            "Epoch 440/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9837 - val_loss: 1.8342 - val_accuracy: 0.8457\n",
            "Epoch 441/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0301 - accuracy: 0.9837 - val_loss: 1.8234 - val_accuracy: 0.8455\n",
            "Epoch 442/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9835 - val_loss: 1.8422 - val_accuracy: 0.8441\n",
            "Epoch 443/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9835 - val_loss: 1.8377 - val_accuracy: 0.8450\n",
            "Epoch 444/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9834 - val_loss: 1.8442 - val_accuracy: 0.8454\n",
            "Epoch 445/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9835 - val_loss: 1.8559 - val_accuracy: 0.8448\n",
            "Epoch 446/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 1.8399 - val_accuracy: 0.8450\n",
            "Epoch 447/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0306 - accuracy: 0.9834 - val_loss: 1.8343 - val_accuracy: 0.8454\n",
            "Epoch 448/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0306 - accuracy: 0.9832 - val_loss: 1.8333 - val_accuracy: 0.8440\n",
            "Epoch 449/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0373 - accuracy: 0.9816 - val_loss: 1.7192 - val_accuracy: 0.8439\n",
            "Epoch 450/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0433 - accuracy: 0.9801 - val_loss: 1.6853 - val_accuracy: 0.8432\n",
            "Epoch 451/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0354 - accuracy: 0.9825 - val_loss: 1.7311 - val_accuracy: 0.8436\n",
            "Epoch 452/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0317 - accuracy: 0.9838 - val_loss: 1.7437 - val_accuracy: 0.8448\n",
            "Epoch 453/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0306 - accuracy: 0.9839 - val_loss: 1.7619 - val_accuracy: 0.8436\n",
            "Epoch 454/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9840 - val_loss: 1.8009 - val_accuracy: 0.8452\n",
            "Epoch 455/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9840 - val_loss: 1.8058 - val_accuracy: 0.8446\n",
            "Epoch 456/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9838 - val_loss: 1.8286 - val_accuracy: 0.8446\n",
            "Epoch 457/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9839 - val_loss: 1.8490 - val_accuracy: 0.8442\n",
            "Epoch 458/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9837 - val_loss: 1.8450 - val_accuracy: 0.8447\n",
            "Epoch 459/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9836 - val_loss: 1.8499 - val_accuracy: 0.8447\n",
            "Epoch 460/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9836 - val_loss: 1.8661 - val_accuracy: 0.8440\n",
            "Epoch 461/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9836 - val_loss: 1.8741 - val_accuracy: 0.8446\n",
            "Epoch 462/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9835 - val_loss: 1.8791 - val_accuracy: 0.8441\n",
            "Epoch 463/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9835 - val_loss: 1.8767 - val_accuracy: 0.8440\n",
            "Epoch 464/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0303 - accuracy: 0.9834 - val_loss: 1.8917 - val_accuracy: 0.8441\n",
            "Epoch 465/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 1.8722 - val_accuracy: 0.8447\n",
            "Epoch 466/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0333 - accuracy: 0.9826 - val_loss: 1.7567 - val_accuracy: 0.8420\n",
            "Epoch 467/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0438 - accuracy: 0.9800 - val_loss: 1.6968 - val_accuracy: 0.8440\n",
            "Epoch 468/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0369 - accuracy: 0.9822 - val_loss: 1.7221 - val_accuracy: 0.8430\n",
            "Epoch 469/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0325 - accuracy: 0.9834 - val_loss: 1.6987 - val_accuracy: 0.8423\n",
            "Epoch 470/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0308 - accuracy: 0.9839 - val_loss: 1.7793 - val_accuracy: 0.8443\n",
            "Epoch 471/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9839 - val_loss: 1.7904 - val_accuracy: 0.8424\n",
            "Epoch 472/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0300 - accuracy: 0.9840 - val_loss: 1.8199 - val_accuracy: 0.8434\n",
            "Epoch 473/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0299 - accuracy: 0.9840 - val_loss: 1.8293 - val_accuracy: 0.8435\n",
            "Epoch 474/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0299 - accuracy: 0.9839 - val_loss: 1.8346 - val_accuracy: 0.8438\n",
            "Epoch 475/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9839 - val_loss: 1.8556 - val_accuracy: 0.8436\n",
            "Epoch 476/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9837 - val_loss: 1.8525 - val_accuracy: 0.8432\n",
            "Epoch 477/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9836 - val_loss: 1.8720 - val_accuracy: 0.8433\n",
            "Epoch 478/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0302 - accuracy: 0.9836 - val_loss: 1.8714 - val_accuracy: 0.8430\n",
            "Epoch 479/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0303 - accuracy: 0.9835 - val_loss: 1.8778 - val_accuracy: 0.8434\n",
            "Epoch 480/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0306 - accuracy: 0.9835 - val_loss: 1.8735 - val_accuracy: 0.8439\n",
            "Epoch 481/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0349 - accuracy: 0.9826 - val_loss: 1.7600 - val_accuracy: 0.8424\n",
            "Epoch 482/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0397 - accuracy: 0.9813 - val_loss: 1.7201 - val_accuracy: 0.8436\n",
            "Epoch 483/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0348 - accuracy: 0.9828 - val_loss: 1.7688 - val_accuracy: 0.8454\n",
            "Epoch 484/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0317 - accuracy: 0.9836 - val_loss: 1.7796 - val_accuracy: 0.8440\n",
            "Epoch 485/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0305 - accuracy: 0.9841 - val_loss: 1.8122 - val_accuracy: 0.8440\n",
            "Epoch 486/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0301 - accuracy: 0.9840 - val_loss: 1.8107 - val_accuracy: 0.8444\n",
            "Epoch 487/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9840 - val_loss: 1.8374 - val_accuracy: 0.8444\n",
            "Epoch 488/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9840 - val_loss: 1.8480 - val_accuracy: 0.8446\n",
            "Epoch 489/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9838 - val_loss: 1.8629 - val_accuracy: 0.8449\n",
            "Epoch 490/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9837 - val_loss: 1.8686 - val_accuracy: 0.8448\n",
            "Epoch 491/500\n",
            "282/282 [==============================] - 10s 35ms/step - loss: 0.0302 - accuracy: 0.9838 - val_loss: 1.8643 - val_accuracy: 0.8442\n",
            "Epoch 492/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0303 - accuracy: 0.9836 - val_loss: 1.8801 - val_accuracy: 0.8443\n",
            "Epoch 493/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0310 - accuracy: 0.9836 - val_loss: 1.8480 - val_accuracy: 0.8427\n",
            "Epoch 494/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0346 - accuracy: 0.9826 - val_loss: 1.8049 - val_accuracy: 0.8433\n",
            "Epoch 495/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0350 - accuracy: 0.9826 - val_loss: 1.7486 - val_accuracy: 0.8427\n",
            "Epoch 496/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0327 - accuracy: 0.9832 - val_loss: 1.7963 - val_accuracy: 0.8432\n",
            "Epoch 497/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0312 - accuracy: 0.9838 - val_loss: 1.7987 - val_accuracy: 0.8434\n",
            "Epoch 498/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0304 - accuracy: 0.9839 - val_loss: 1.8372 - val_accuracy: 0.8430\n",
            "Epoch 499/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0300 - accuracy: 0.9839 - val_loss: 1.8413 - val_accuracy: 0.8435\n",
            "Epoch 500/500\n",
            "282/282 [==============================] - 10s 36ms/step - loss: 0.0299 - accuracy: 0.9840 - val_loss: 1.8711 - val_accuracy: 0.8428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jByV10j1KHmJ"
      },
      "source": [
        "# 進行預測並提交結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "susyUDtlNMSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "9983ea32-db60-403b-9247-2adfbb277f53"
      },
      "source": [
        "print(\"Use raw test csv\")\n",
        "test = pd.read_csv(TEST_CSV_PATH, index_col=0)\n",
        "test.fillna('UNKNOWN', inplace=True)\n",
        "test['title1_tokenized'] = parallelize(test.loc[:, 'title1_zh'], process)\n",
        "test['title2_tokenized'] = parallelize(test.loc[:, 'title2_zh'], process)\n",
        "test.fillna('UNKNOWN', inplace=True)\n",
        "test.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use raw test csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 1.295 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Loading model cost 1.311 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 1.148 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Loading model cost 1.202 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tid1</th>\n",
              "      <th>tid2</th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>title1_en</th>\n",
              "      <th>title2_en</th>\n",
              "      <th>title1_tokenized</th>\n",
              "      <th>title2_tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>321187</th>\n",
              "      <td>167562</td>\n",
              "      <td>59521</td>\n",
              "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
              "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
              "      <td>egypt 's presidential election failed to win m...</td>\n",
              "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
              "      <td>萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大</td>\n",
              "      <td>辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321190</th>\n",
              "      <td>167564</td>\n",
              "      <td>91315</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "      <td>10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321189</th>\n",
              "      <td>167563</td>\n",
              "      <td>167564</td>\n",
              "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>Will the United States wage war on Iraq withou...</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗</td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          tid1  ...                        title2_tokenized\n",
              "id              ...                                        \n",
              "321187  167562  ...    辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢\n",
              "321190  167564  ...  10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国\n",
              "321189  167563  ...              萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMUzP9A4JFgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "05a2a3fe-2081-46dc-a5d0-5f1bceb75d66"
      },
      "source": [
        "test.insert(4, column=\"label\", value=\"\")\n",
        "test.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tid1</th>\n",
              "      <th>tid2</th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>label</th>\n",
              "      <th>title1_en</th>\n",
              "      <th>title2_en</th>\n",
              "      <th>title1_tokenized</th>\n",
              "      <th>title2_tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>321187</th>\n",
              "      <td>167562</td>\n",
              "      <td>59521</td>\n",
              "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
              "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
              "      <td></td>\n",
              "      <td>egypt 's presidential election failed to win m...</td>\n",
              "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
              "      <td>萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大</td>\n",
              "      <td>辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321190</th>\n",
              "      <td>167564</td>\n",
              "      <td>91315</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
              "      <td></td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "      <td>10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321189</th>\n",
              "      <td>167563</td>\n",
              "      <td>167564</td>\n",
              "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td></td>\n",
              "      <td>Will the United States wage war on Iraq withou...</td>\n",
              "      <td>A message from Saddam Hussein after he was cap...</td>\n",
              "      <td>萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗</td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          tid1  ...                        title2_tokenized\n",
              "id              ...                                        \n",
              "321187  167562  ...    辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢\n",
              "321190  167564  ...  10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国\n",
              "321189  167563  ...              萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_xFpGdHDcm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "18e91faf-8d1e-4e82-9c66-3ed9cf30d14b"
      },
      "source": [
        "cols = ['title1_zh', \n",
        "        'title2_zh', \n",
        "        'label',\n",
        "        'title1_tokenized',\n",
        "        'title2_tokenized']\n",
        "test = test.loc[:, cols]\n",
        "test.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title1_zh</th>\n",
              "      <th>title2_zh</th>\n",
              "      <th>label</th>\n",
              "      <th>title1_tokenized</th>\n",
              "      <th>title2_tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>321187</th>\n",
              "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
              "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
              "      <td></td>\n",
              "      <td>萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大</td>\n",
              "      <td>辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321190</th>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
              "      <td></td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "      <td>10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321189</th>\n",
              "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
              "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
              "      <td></td>\n",
              "      <td>萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗</td>\n",
              "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              title1_zh  ...                        title2_tokenized\n",
              "id                                       ...                                        \n",
              "321187  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  ...    辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢\n",
              "321190              萨达姆被捕后告诫美国的一句话，发人深思  ...  10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国\n",
              "321189    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗  ...              萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZq_r8VNMSt"
      },
      "source": [
        "# 將詞彙序列轉為索引數字的序列\n",
        "x1_test = tokenizer \\\n",
        "    .texts_to_sequences(\n",
        "        test.title1_tokenized)\n",
        "x2_test = tokenizer \\\n",
        "    .texts_to_sequences(\n",
        "        test.title2_tokenized)\n",
        "\n",
        "# 為數字序列加入 zero padding\n",
        "x1_test = keras \\\n",
        "    .preprocessing \\\n",
        "    .sequence \\\n",
        "    .pad_sequences(\n",
        "        x1_test, \n",
        "        maxlen=MAX_SEQUENCE_LENGTH)\n",
        "x2_test = keras \\\n",
        "    .preprocessing \\\n",
        "    .sequence \\\n",
        "    .pad_sequences(\n",
        "        x2_test, \n",
        "        maxlen=MAX_SEQUENCE_LENGTH)    \n",
        "\n",
        "# 利用已訓練的模型做預測\n",
        "predictions = model.predict(\n",
        "    [x1_test, x2_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgujrcjBNMSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a198387-78a5-42c0-fca8-9183053bc12e"
      },
      "source": [
        "# 從模型得到的預測結果\n",
        "# 會回傳給我們 3 個分類的機率值\n",
        "predictions[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0000000e+00, 1.7628143e-19, 4.3848384e-16],\n",
              "       [1.0000000e+00, 5.4379916e-21, 1.5212559e-12],\n",
              "       [9.9990010e-01, 9.9930898e-05, 5.1073485e-13],\n",
              "       [1.0000000e+00, 1.1076470e-22, 2.5842076e-19],\n",
              "       [1.0000000e+00, 1.0157090e-15, 1.1334577e-11]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz68v3VXNMSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8ebab330-7aff-4753-a0f1-de1e4bbbdaf6"
      },
      "source": [
        "# 將機率值最大的類別當作答案，並將這個結果轉回對應的文本標籤\n",
        "index_to_label = {v: k for k, v in label_to_index.items()}\n",
        "\n",
        "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
        "\n",
        "submission = test \\\n",
        "    .loc[:, ['Category']] \\\n",
        "    .reset_index()\n",
        "\n",
        "submission.columns = ['Id', 'Category']\n",
        "submission.to_csv('/content/drive/MyDrive/nlp_final/submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>321187</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>321190</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>321189</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>321193</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>321191</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Id   Category\n",
              "0  321187  unrelated\n",
              "1  321190  unrelated\n",
              "2  321189  unrelated\n",
              "3  321193  unrelated\n",
              "4  321191  unrelated"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}